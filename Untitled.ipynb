{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import  functional as F\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    '''\n",
    "    实现子module: Residual Block\n",
    "    '''\n",
    "    def __init__(self, inchannel, outchannel, stride=1, shortcut=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "                nn.Conv2d(inchannel,outchannel,3,stride, 1,bias=False),\n",
    "                nn.BatchNorm2d(outchannel),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(outchannel,outchannel,3,1,1,bias=False),\n",
    "                nn.BatchNorm2d(outchannel) )\n",
    "        self.right = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        residual = x if self.right is None else self.right(x)\n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "\n",
    "    def save(self, dir, conv_num, bn_num):\n",
    "        conv1 = self.left[0].weight.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"conv{}_weight.npy\".format(str(conv_num))), conv1)\n",
    "        conv_num += 1\n",
    "\n",
    "        bn1w = self.left[1].weight.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_weight.npy\".format(str(bn_num))), bn1w)\n",
    "        bn1b = self.left[1].bias.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_bias.npy\".format(str(bn_num))), bn1b)\n",
    "        bn1m = self.left[1].running_mean.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_mean.npy\".format(str(bn_num))), bn1m)\n",
    "        bn1v = self.left[1].running_var.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_var.npy\".format(str(bn_num))), bn1v)\n",
    "        bn_num += 1\n",
    "\n",
    "        conv2 = self.left[3].weight.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"conv{}_weight.npy\".format(str(conv_num))), conv2)\n",
    "        conv_num += 1\n",
    "\n",
    "        bn2w = self.left[4].weight.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_weight.npy\".format(str(bn_num))), bn2w)\n",
    "        bn2b = self.left[4].bias.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_bias.npy\".format(str(bn_num))), bn2b)\n",
    "        bn2m = self.left[4].running_mean.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_mean.npy\".format(str(bn_num))), bn2m)\n",
    "        bn2v = self.left[4].running_var.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_var.npy\".format(str(bn_num))), bn2v)\n",
    "        bn_num += 1\n",
    "\n",
    "        if self.right is not None:\n",
    "            conv3 = self.right[0].weight.data.detach().cpu().numpy()\n",
    "            np.save(os.path.join(dir, \"conv{}_weight.npy\".format(str(conv_num))), conv3)\n",
    "            conv_num += 1\n",
    "\n",
    "            bn3w = self.right[1].weight.data.detach().cpu().numpy()\n",
    "            np.save(os.path.join(dir, \"bn{}_weight.npy\".format(str(bn_num))), bn3w)\n",
    "            bn3b = self.right[1].bias.data.detach().cpu().numpy()\n",
    "            np.save(os.path.join(dir, \"bn{}_bias.npy\".format(str(bn_num))), bn3b)\n",
    "            bn3m = self.right[1].running_mean.data.detach().cpu().numpy()\n",
    "            np.save(os.path.join(dir, \"bn{}_mean.npy\".format(str(bn_num))), bn3m)\n",
    "            bn3v = self.right[1].running_var.data.detach().cpu().numpy()\n",
    "            np.save(os.path.join(dir, \"bn{}_var.npy\".format(str(bn_num))), bn3v)\n",
    "            bn_num += 1\n",
    "\n",
    "        return conv_num, bn_num\n",
    "    \n",
    "    def load(self, dir, conv_num, bn_num):\n",
    "        conv1 = np.load(os.path.join(dir, \"conv{}_weight.npy\".format(conv_num)))\n",
    "        self.left[0].weight.data = torch.from_numpy(conv1)\n",
    "        conv_num += 1\n",
    "\n",
    "        bn1w = np.load(os.path.join(dir, \"bn{}_weight.npy\".format(bn_num)))\n",
    "        self.left[1].weight.data = torch.from_numpy(bn1w)\n",
    "        bn1b = np.load(os.path.join(dir, \"bn{}_bias.npy\".format(bn_num)))\n",
    "        self.left[1].bias.data = torch.from_numpy(bn1b)\n",
    "        bn1m = np.load(os.path.join(dir, \"bn{}_mean.npy\".format(bn_num)))\n",
    "        self.left[1].running_mean.data = torch.from_numpy(bn1m)\n",
    "        bn1v = np.load(os.path.join(dir, \"bn{}_var.npy\".format(bn_num)))\n",
    "        self.left[1].running_var.data = torch.from_numpy(bn1v)\n",
    "        bn_num += 1\n",
    "\n",
    "        conv2 = np.load(os.path.join(dir, \"conv{}_weight.npy\".format(conv_num)))\n",
    "        self.left[3].weight.data = torch.from_numpy(conv2)\n",
    "        conv_num += 1\n",
    "\n",
    "        bn2w = np.load(os.path.join(dir, \"bn{}_weight.npy\".format(bn_num)))\n",
    "        self.left[4].weight.data = torch.from_numpy(bn2w)\n",
    "        bn2b = np.load(os.path.join(dir, \"bn{}_bias.npy\".format(bn_num)))\n",
    "        self.left[4].bias.data = torch.from_numpy(bn2b)\n",
    "        bn2m = np.load(os.path.join(dir, \"bn{}_mean.npy\".format(bn_num)))\n",
    "        self.left[4].running_mean.data = torch.from_numpy(bn2m)\n",
    "        bn2v = np.load(os.path.join(dir, \"bn{}_var.npy\".format(bn_num)))\n",
    "        self.left[4].running_var.data = torch.from_numpy(bn2v)\n",
    "        bn_num += 1\n",
    "\n",
    "        if self.right is not None:\n",
    "            conv3 = np.load(os.path.join(dir, \"conv{}_weight.npy\".format(conv_num)))\n",
    "            self.right[0].weight.data = torch.from_numpy(conv3)\n",
    "            conv_num += 1\n",
    "\n",
    "            bn3w = np.load(os.path.join(dir, \"bn{}_weight.npy\".format(bn_num)))\n",
    "            self.right[1].weight.data = torch.from_numpy(bn3w)\n",
    "            bn3b = np.load(os.path.join(dir, \"bn{}_bias.npy\".format(bn_num)))\n",
    "            self.right[1].bias.data = torch.from_numpy(bn3b)\n",
    "            bn3m = np.load(os.path.join(dir, \"bn{}_mean.npy\".format(bn_num)))\n",
    "            self.right[1].running_mean.data = torch.from_numpy(bn3m)\n",
    "            bn3v = np.load(os.path.join(dir, \"bn{}_var.npy\".format(bn_num)))\n",
    "            self.right[1].running_var.data = torch.from_numpy(bn3v)\n",
    "            bn_num += 1\n",
    "\n",
    "        return conv_num, bn_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_layer:\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_h, kernel_w, same = True, stride = 1, shift = True):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_h = kernel_h\n",
    "        self.kernel_w = kernel_w\n",
    "        self.same = same\n",
    "        self.stride = stride\n",
    "        self.shift = shift\n",
    "\n",
    "        self.init_param()\n",
    "\n",
    "    def init_param(self):\n",
    "        self.kernel = np.random.uniform(\n",
    "            low = -np.sqrt(6.0/(self.out_channels + self.in_channels * self.kernel_h * self.kernel_w)),\n",
    "            high = np.sqrt(6.0/(self.in_channels + self.out_channels * self.kernel_h * self.kernel_w)),\n",
    "            size = (self.out_channels, self.in_channels, self.kernel_h, self.kernel_w)\n",
    "        )\n",
    "        self.bias = np.zeros([self.out_channels]) if self.shift else None\n",
    "\n",
    "    @staticmethod\n",
    "    def pad(in_tensor, pad_h, pad_w):\n",
    "        batch_num = in_tensor.shape[0]\n",
    "        in_channels = in_tensor.shape[1]\n",
    "        in_h = in_tensor.shape[2]\n",
    "        in_w = in_tensor.shape[3]\n",
    "        padded = np.zeros([batch_num, in_channels, in_h + 2*pad_h, in_w + 2*pad_w])\n",
    "        padded[:, :, pad_h:pad_h+in_h, pad_w:pad_w+in_w] = in_tensor\n",
    "        return padded\n",
    "    \n",
    "    @staticmethod\n",
    "    def convolution(in_tensor, kernel, stride = 1, dilate = 1):\n",
    "        batch_num = in_tensor.shape[0]\n",
    "        in_channels = in_tensor.shape[1]\n",
    "        in_h = in_tensor.shape[2]\n",
    "        in_w = in_tensor.shape[3]\n",
    "        out_channels = kernel.shape[0]\n",
    "        assert kernel.shape[1] == in_channels\n",
    "        kernel_h = kernel.shape[2]\n",
    "        kernel_w = kernel.shape[3]\n",
    "        \n",
    "        out_h = int((in_h - kernel_h + 1)/stride)\n",
    "        out_w = int((in_w - kernel_w + 1)/stride)\n",
    "        \n",
    "        kernel = kernel.reshape(out_channels, -1)\n",
    "        \n",
    "        extend_in = np.zeros([in_channels*kernel_h*kernel_w, batch_num*out_h*out_w])\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                part_in = in_tensor[:, :, i*stride:i*stride+kernel_h, j*stride:j*stride+kernel_w].reshape(batch_num, -1)\n",
    "                extend_in[:, (i*out_w+j)*batch_num:(i*out_w+j+1)*batch_num] = part_in.T\n",
    "        \n",
    "        out_tensor = np.dot(kernel, extend_in)\n",
    "        out_tensor = out_tensor.reshape(out_channels, out_h*out_w, batch_num)\n",
    "        out_tensor = out_tensor.transpose(2,0,1).reshape(batch_num, out_channels, out_h, out_w) \n",
    "        \n",
    "        return out_tensor\n",
    "    \n",
    "    def forward(self, in_tensor):\n",
    "        if self.same:\n",
    "            in_tensor = conv_layer.pad(in_tensor, int((self.kernel_h-1)/2), int((self.kernel_w-1)/2))\n",
    "        \n",
    "        self.in_tensor = in_tensor.copy()\n",
    "        \n",
    "        self.out_tensor = conv_layer.convolution(in_tensor, self.kernel, self.stride)\n",
    "\n",
    "        if self.shift:\n",
    "            self.out_tensor += self.bias.reshape(1,self.out_channels,1,1)\n",
    "\n",
    "        return self.out_tensor\n",
    "    \n",
    "    def backward(self, out_diff_tensor, lr):\n",
    "        assert out_diff_tensor.shape == self.out_tensor.shape\n",
    "\n",
    "        if self.shift:\n",
    "            bias_diff = np.sum(out_diff_tensor, axis = (0,2,3)).reshape(self.bias.shape)\n",
    "            self.bias -= lr * bias_diff\n",
    "        \n",
    "        batch_num = out_diff_tensor.shape[0]\n",
    "        out_channels = out_diff_tensor.shape[1]\n",
    "        out_h = out_diff_tensor.shape[2]\n",
    "        out_w = out_diff_tensor.shape[3]\n",
    "        extend_out = np.zeros([batch_num, out_channels, out_h, out_w, self.stride * self.stride])\n",
    "        extend_out[:, :, :, :, 0] = out_diff_tensor\n",
    "        extend_out = extend_out.reshape(batch_num, out_channels, out_h, out_w, self.stride, self.stride)\n",
    "        extend_out = extend_out.transpose(0,1,2,4,3,5).reshape(batch_num, out_channels, out_h*self.stride, out_w*self.stride)\n",
    "\n",
    "        kernel_diff = conv_layer.convolution(self.in_tensor.transpose(1,0,2,3), extend_out.transpose(1,0,2,3))\n",
    "        kernel_diff = kernel_diff.transpose(1,0,2,3)\n",
    "        \n",
    "        padded = conv_layer.pad(extend_out, self.kernel_h-1, self.kernel_w-1)\n",
    "        kernel_trans = self.kernel.reshape(self.out_channels, self.in_channels, self.kernel_h*self.kernel_w)\n",
    "        kernel_trans = kernel_trans[:,:,::-1].reshape(self.kernel.shape)\n",
    "        self.in_diff_tensor = conv_layer.convolution(padded, kernel_trans.transpose(1,0,2,3))\n",
    "        if self.same:\n",
    "            pad_h = int((self.kernel_h-1)/2)\n",
    "            pad_w = int((self.kernel_w-1)/2)\n",
    "            self.in_diff_tensor = self.in_diff_tensor[:, :, pad_h:-pad_h, pad_w:-pad_w]\n",
    "            \n",
    "        self.kernel -= lr * kernel_diff\n",
    "\n",
    "    def save(self, path, conv_num):\n",
    "        if os.path.exists(path) == False:\n",
    "            os.mkdir(path)\n",
    "\n",
    "        np.save(os.path.join(path, \"conv{}_weight.npy\".format(conv_num)), self.kernel)\n",
    "        if self.shift:\n",
    "            np.save(os.path.join(path, \"conv{}_bias.npy\".format(conv_num)), self.bias)\n",
    "        \n",
    "        return conv_num + 1\n",
    "\n",
    "    def load(self, path, conv_num):\n",
    "        assert os.path.exists(path)\n",
    "\n",
    "        self.kernel = np.load(os.path.join(path, \"conv{}_weight.npy\".format(conv_num)))\n",
    "        if self.shift:\n",
    "            self.bias = np.load(os.path.join(path, \"conv{}_bias.npy\").format(conv_num))\n",
    "        \n",
    "        return conv_num + 1\n",
    "    \n",
    "class relu:\n",
    "\n",
    "    def forward(self, in_tensor):\n",
    "        self.in_tensor = in_tensor.copy()\n",
    "        self.out_tensor = in_tensor.copy()\n",
    "        self.out_tensor[self.in_tensor < 0.0] = 0.0\n",
    "        return self.out_tensor\n",
    "\n",
    "    def backward(self, out_diff_tensor, lr = 0):\n",
    "        assert self.out_tensor.shape == out_diff_tensor.shape\n",
    "        self.in_diff_tensor = out_diff_tensor.copy()\n",
    "        self.in_diff_tensor[self.in_tensor < 0.0] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "class bn_layer:\n",
    "\n",
    "    def __init__(self, neural_num, moving_rate = 0.1):\n",
    "        self.gamma = np.random.uniform(low=0, high=1, size=neural_num)\n",
    "        self.bias = np.zeros([neural_num])\n",
    "        self.moving_avg = np.zeros([neural_num])\n",
    "        self.moving_var = np.ones([neural_num])\n",
    "        self.neural_num = neural_num\n",
    "        self.moving_rate = moving_rate\n",
    "        self.is_train = True\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def train(self):\n",
    "        self.is_train = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.is_train = False\n",
    "\n",
    "    def forward(self, in_tensor):\n",
    "        assert in_tensor.shape[1] == self.neural_num\n",
    "\n",
    "        self.in_tensor = in_tensor.copy()\n",
    "\n",
    "        if self.is_train:\n",
    "            mean = in_tensor.mean(axis=(0,2,3))\n",
    "            var = in_tensor.var(axis=(0,2,3))\n",
    "            self.moving_avg = mean * self.moving_rate + (1 - self.moving_rate) * self.moving_avg\n",
    "            self.moving_var = var * self.moving_rate + (1 - self.moving_rate) * self.moving_var\n",
    "            self.var = var\n",
    "            self.mean = mean\n",
    "        else:\n",
    "            mean = self.moving_avg\n",
    "            var = self.moving_var\n",
    "\n",
    "        self.normalized = (in_tensor - mean.reshape(1,-1,1,1)) / np.sqrt(var.reshape(1,-1,1,1)+self.epsilon)\n",
    "        out_tensor = self.gamma.reshape(1,-1,1,1) * self.normalized + self.bias.reshape(1,-1,1,1)\n",
    "\n",
    "        return out_tensor\n",
    "\n",
    "    def backward(self, out_diff_tensor, lr):\n",
    "        assert out_diff_tensor.shape == self.in_tensor.shape\n",
    "        assert self.is_train\n",
    "\n",
    "        m = self.in_tensor.shape[0] * self.in_tensor.shape[2] * self.in_tensor.shape[3]\n",
    "\n",
    "        normalized_diff = self.gamma.reshape(1,-1,1,1) * out_diff_tensor\n",
    "        var_diff = -0.5 * np.sum(normalized_diff*self.normalized, axis=(0,2,3)) / (self.var + self.epsilon)\n",
    "        mean_diff = -1.0 * np.sum(normalized_diff, axis=(0,2,3)) / np.sqrt(self.var + self.epsilon)\n",
    "        in_diff_tensor1 = normalized_diff / np.sqrt(self.var.reshape(1,-1,1,1)+self.epsilon)\n",
    "        in_diff_tensor2 = var_diff.reshape(1,-1,1,1) * (self.in_tensor - self.mean.reshape(1,-1,1,1)) * 2 / m\n",
    "        in_diff_tensor3 = mean_diff.reshape(1,-1,1,1) / m\n",
    "        self.in_diff_tensor = in_diff_tensor1 + in_diff_tensor2 + in_diff_tensor3\n",
    "\n",
    "        gamma_diff = np.sum(self.normalized * out_diff_tensor, axis=(0,2,3))\n",
    "        self.gamma -= lr * gamma_diff\n",
    "\n",
    "        bias_diff = np.sum(out_diff_tensor, axis=(0,2,3))\n",
    "        self.bias -= lr * bias_diff \n",
    "\n",
    "    def save(self, path, bn_num):\n",
    "        if os.path.exists(path) == False:\n",
    "            os.mkdir(path)\n",
    "\n",
    "        np.save(os.path.join(path, \"bn{}_weight.npy\".format(bn_num)), self.gamma)\n",
    "        np.save(os.path.join(path, \"bn{}_bias.npy\".format(bn_num)), self.bias)\n",
    "        np.save(os.path.join(path, \"bn{}_mean.npy\".format(bn_num)), self.moving_avg)\n",
    "        np.save(os.path.join(path, \"bn{}_var.npy\".format(bn_num)), self.moving_var)\n",
    "\n",
    "        return bn_num + 1\n",
    "\n",
    "    def load(self, path, bn_num):\n",
    "        assert os.path.exists(path)\n",
    "\n",
    "        self.gamma = np.load(os.path.join(path, \"bn{}_weight.npy\".format(bn_num)))\n",
    "        self.bias = np.load(os.path.join(path, \"bn{}_bias.npy\".format(bn_num)))\n",
    "        self.moving_avg = np.load(os.path.join(path, \"bn{}_mean.npy\".format(bn_num)))\n",
    "        self.moving_var = np.load(os.path.join(path, \"bn{}_var.npy\".format(bn_num)))\n",
    "\n",
    "        return bn_num + 1\n",
    "\n",
    "class ResBlock:\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, shortcut=None):\n",
    "        self.path1 = [\n",
    "            conv_layer(in_channels, out_channels, 3, 3, stride = stride, shift=False),\n",
    "            bn_layer(out_channels),\n",
    "            relu(),\n",
    "            conv_layer(out_channels, out_channels, 3, 3, shift=False),\n",
    "            bn_layer(out_channels)\n",
    "        ]\n",
    "        self.path2 = shortcut\n",
    "        self.relu = relu()\n",
    "    \n",
    "    def train(self):\n",
    "        self.path1[1].train()\n",
    "        self.path1[4].train()\n",
    "        if self.path2 is not None:\n",
    "            self.path2[1].train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.path1[1].eval()\n",
    "        self.path1[4].eval()\n",
    "        if self.path2 is not None:\n",
    "            self.path2[1].eval()\n",
    "\n",
    "    def forward(self, in_tensor):\n",
    "        x1 = in_tensor.copy()\n",
    "        x2 = in_tensor.copy()\n",
    "\n",
    "        for l in self.path1:\n",
    "            x1 = l.forward(x1)\n",
    "        if self.path2 is not None:\n",
    "            for l in self.path2:\n",
    "                x2 = l.forward(x2)\n",
    "        self.out_tensor = self.relu.forward(x1+x2)\n",
    "\n",
    "        return self.out_tensor\n",
    "\n",
    "\n",
    "    def save(self, path, conv_num, bn_num):\n",
    "        conv_num = self.path1[0].save(path, conv_num)\n",
    "        bn_num = self.path1[1].save(path, bn_num)\n",
    "        conv_num = self.path1[3].save(path, conv_num)\n",
    "        bn_num = self.path1[4].save(path, bn_num)\n",
    "\n",
    "        if self.path2 is not None:\n",
    "            conv_num = self.path2[0].save(path, conv_num)\n",
    "            bn_num = self.path2[1].save(path, bn_num)\n",
    "\n",
    "        return conv_num, bn_num\n",
    "\n",
    "    def load(self, path, conv_num, bn_num):\n",
    "        conv_num = self.path1[0].load(path, conv_num)\n",
    "        bn_num = self.path1[1].load(path, bn_num)\n",
    "        conv_num = self.path1[3].load(path, conv_num)\n",
    "        bn_num = self.path1[4].load(path, bn_num)\n",
    "\n",
    "        if self.path2 is not None:\n",
    "            conv_num = self.path2[0].load(path, conv_num)\n",
    "            bn_num = self.path2[1].load(path, bn_num)\n",
    "\n",
    "        return conv_num, bn_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from components import *\n",
    "from network import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_n = np.random.randn(2,3,64,64)\n",
    "in_t = torch.from_numpy(in_n).float()\n",
    "in_t.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1 = resnet34(20)\n",
    "bn2 = ResNet(20)\n",
    "bn2.save(\"model7\")\n",
    "bn1.load(\"model7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 16, 16]) torch.Size([2, 64, 16, 16])\n",
      "torch.Size([2, 64, 16, 16]) torch.Size([2, 64, 16, 16])\n",
      "torch.Size([2, 64, 16, 16]) torch.Size([2, 64, 16, 16])\n",
      "torch.Size([2, 128, 8, 8]) torch.Size([2, 128, 8, 8])\n",
      "torch.Size([2, 128, 8, 8]) torch.Size([2, 128, 8, 8])\n",
      "torch.Size([2, 128, 8, 8]) torch.Size([2, 128, 8, 8])\n",
      "torch.Size([2, 128, 8, 8]) torch.Size([2, 128, 8, 8])\n",
      "torch.Size([2, 256, 4, 4]) torch.Size([2, 256, 4, 4])\n",
      "torch.Size([2, 256, 4, 4]) torch.Size([2, 256, 4, 4])\n",
      "torch.Size([2, 256, 4, 4]) torch.Size([2, 256, 4, 4])\n",
      "torch.Size([2, 256, 4, 4]) torch.Size([2, 256, 4, 4])\n",
      "torch.Size([2, 256, 4, 4]) torch.Size([2, 256, 4, 4])\n",
      "torch.Size([2, 256, 4, 4]) torch.Size([2, 256, 4, 4])\n",
      "torch.Size([2, 512, 2, 2]) torch.Size([2, 512, 2, 2])\n",
      "torch.Size([2, 512, 2, 2]) torch.Size([2, 512, 2, 2])\n",
      "torch.Size([2, 512, 2, 2]) torch.Size([2, 512, 2, 2])\n",
      "[[0.47470274 0.53944822 0.51017137 0.30650067 0.49524024 0.53134661\n",
      "  0.50256288 0.66426079 0.46259805 0.37517949 0.54271628 0.58348111\n",
      "  0.57146094 0.49453737 0.55289317 0.70502303 0.51594349 0.50440423\n",
      "  0.56996566 0.33119192]\n",
      " [0.5182852  0.51224784 0.4897964  0.41003625 0.52212007 0.48751289\n",
      "  0.49130647 0.5341233  0.45714583 0.5443553  0.48447266 0.53902924\n",
      "  0.51609172 0.4511515  0.43835027 0.66192429 0.52721995 0.52354093\n",
      "  0.45855361 0.33032541]]\n",
      "tensor([[0.4747, 0.5394, 0.5102, 0.3065, 0.4952, 0.5313, 0.5026, 0.6643, 0.4626,\n",
      "         0.3752, 0.5427, 0.5835, 0.5715, 0.4945, 0.5529, 0.7050, 0.5159, 0.5044,\n",
      "         0.5700, 0.3312],\n",
      "        [0.5183, 0.5122, 0.4898, 0.4100, 0.5221, 0.4875, 0.4913, 0.5341, 0.4571,\n",
      "         0.5444, 0.4845, 0.5390, 0.5161, 0.4512, 0.4384, 0.6619, 0.5272, 0.5235,\n",
      "         0.4586, 0.3303]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyzustc/applications/anaconda3/envs/pytorch4/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "out1 = bn1.forward(in_n)\n",
    "out2 = bn2(in_t)\n",
    "print(out1)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 5.90819158e-03  3.43736262e-03  1.50333080e-03 ...  2.64158754e-03\n",
      "     3.52910164e-05 -9.12694691e-04]\n",
      "   [ 1.23524310e-02  4.55476877e-03 -4.08180879e-03 ... -5.30715725e-04\n",
      "    -8.11735846e-04 -1.82933754e-03]\n",
      "   [ 5.72507029e-03 -2.77475036e-02  9.37338680e-04 ... -1.62348890e-03\n",
      "     2.00137579e-04 -2.56272212e-03]\n",
      "   ...\n",
      "   [-3.22040804e-03  5.49114252e-03  7.41648161e-03 ...  5.05466426e-03\n",
      "    -2.20717292e-03 -1.28212594e-03]\n",
      "   [-5.76905078e-04  3.01659316e-03  6.66994189e-05 ...  1.19905746e-03\n",
      "     1.20188261e-03  4.41400115e-03]\n",
      "   [-2.16947151e-03  5.78977073e-03 -1.69361598e-03 ...  3.73768499e-03\n",
      "     2.63824601e-03 -7.48866818e-04]]\n",
      "\n",
      "  [[-2.60327317e-03  2.23995162e-03 -3.47933889e-04 ... -3.34248006e-03\n",
      "    -5.34795291e-03  1.63161807e-03]\n",
      "   [-1.68622785e-02  8.84059937e-03  6.17426215e-03 ... -5.87906103e-03\n",
      "     5.10255968e-04 -3.10303440e-03]\n",
      "   [ 1.81166008e-04 -5.10856921e-03 -6.16627417e-03 ...  2.35884178e-03\n",
      "    -2.29381486e-03 -2.69982435e-03]\n",
      "   ...\n",
      "   [-4.37597925e-03 -1.37820145e-02 -2.42599749e-03 ...  3.46901680e-03\n",
      "    -7.35807297e-04 -1.10896813e-03]\n",
      "   [-1.71048962e-03 -2.62051636e-03  2.08433013e-03 ... -7.63337490e-04\n",
      "    -3.13692965e-03 -6.67046254e-05]\n",
      "   [-1.77172840e-03  3.69599161e-03 -4.90700016e-03 ...  3.38093337e-03\n",
      "     1.65839511e-03 -8.72047729e-04]]\n",
      "\n",
      "  [[ 2.03438761e-03  6.84059527e-03  2.22610794e-03 ... -1.54834953e-03\n",
      "     3.10447003e-03  1.50110208e-03]\n",
      "   [ 1.13665384e-03 -7.88194931e-03  7.97396035e-04 ...  2.25211998e-03\n",
      "     8.19131700e-04 -1.32771171e-03]\n",
      "   [-1.53820676e-02  6.08085136e-03 -1.25713872e-02 ... -7.34726394e-03\n",
      "    -2.35361965e-03  1.31234746e-03]\n",
      "   ...\n",
      "   [ 4.09511735e-03 -3.56895139e-03  4.05035799e-03 ... -2.09063604e-04\n",
      "     3.43497478e-03 -5.61109551e-03]\n",
      "   [-1.45996460e-03 -4.16938351e-03  5.93299916e-03 ...  1.87107686e-03\n",
      "    -1.75143518e-04 -3.43064530e-04]\n",
      "   [ 1.83524599e-03  9.74603765e-03  1.54594869e-03 ...  3.03593853e-03\n",
      "     4.87287021e-03  2.32190466e-04]]]\n",
      "\n",
      "\n",
      " [[[-6.05257660e-03  4.20321716e-03 -1.22043291e-02 ... -4.18080595e-03\n",
      "    -3.95972528e-03  4.22627649e-03]\n",
      "   [-5.33245937e-03 -1.71663234e-02 -4.00680388e-03 ... -5.08226093e-04\n",
      "    -4.46139103e-03  4.86736282e-03]\n",
      "   [-1.24586729e-02  9.30014749e-03  6.86042274e-03 ... -1.47799104e-02\n",
      "     6.61187002e-03 -1.84701893e-03]\n",
      "   ...\n",
      "   [-9.01317929e-03 -1.48526863e-03 -2.60410634e-04 ...  2.15873255e-03\n",
      "     1.87538046e-03  2.11538049e-04]\n",
      "   [ 4.46649879e-03  1.24125972e-03  1.12152344e-02 ...  1.06475665e-03\n",
      "     1.02341409e-03 -3.12222360e-04]\n",
      "   [ 7.70744964e-04  7.31996147e-03  7.03808724e-03 ... -2.92233422e-04\n",
      "    -5.13865284e-04 -1.81401640e-03]]\n",
      "\n",
      "  [[ 3.88837432e-03  4.63122534e-03  1.16659744e-02 ...  1.30941001e-02\n",
      "     4.80811175e-03 -4.14027216e-03]\n",
      "   [ 3.77204364e-03  2.07721119e-02  3.00694507e-03 ...  1.02534422e-02\n",
      "    -4.01009501e-03 -6.10364637e-03]\n",
      "   [ 2.27633684e-03  1.06980365e-05  6.14326861e-03 ...  1.23551152e-02\n",
      "     6.02094650e-03 -1.85781281e-03]\n",
      "   ...\n",
      "   [ 1.51716223e-04  6.76154532e-03  4.40029466e-03 ... -5.73776709e-04\n",
      "    -3.18672956e-04 -6.00312396e-04]\n",
      "   [ 4.31697160e-03 -5.12756443e-03 -4.88072397e-04 ... -4.18069280e-03\n",
      "    -7.26951016e-04  3.28414719e-04]\n",
      "   [ 5.14068628e-03  2.02446241e-03  7.03886902e-03 ...  7.41451783e-04\n",
      "     1.81858011e-03  4.21803775e-04]]\n",
      "\n",
      "  [[-4.02888549e-03 -9.43563982e-03  6.22249659e-03 ...  5.41124218e-03\n",
      "    -5.98152963e-04  7.87267423e-03]\n",
      "   [ 2.52232670e-03  9.73213052e-03  1.16395264e-02 ... -8.59018470e-03\n",
      "    -7.50504339e-03 -3.37962529e-03]\n",
      "   [ 9.33180806e-03  1.63184990e-03 -5.55104171e-03 ... -1.24423677e-02\n",
      "     3.04845678e-03 -2.60073925e-03]\n",
      "   ...\n",
      "   [-5.58028129e-03  1.44333643e-03 -1.03094680e-02 ... -7.05158288e-04\n",
      "     7.40518514e-04 -3.15605984e-04]\n",
      "   [ 7.93916389e-04 -2.14511307e-03 -8.78345302e-03 ... -1.36137915e-03\n",
      "    -1.40772038e-03 -3.68544929e-04]\n",
      "   [-7.96303424e-03  1.11324008e-02  8.48598346e-04 ...  1.19996404e-03\n",
      "     1.93541972e-03 -1.15035104e-03]]]]\n"
     ]
    }
   ],
   "source": [
    "k = np.random.uniform(0,1,out1.shape)\n",
    "bn1.backward(k,1)\n",
    "print(bn1.in_diff_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.9043e-03,  3.4333e-03,  1.5049e-03,  ...,  2.6432e-03,\n",
       "            3.6375e-05, -9.1327e-04],\n",
       "          [ 1.2349e-02,  4.5567e-03, -4.0727e-03,  ..., -5.3365e-04,\n",
       "           -8.1134e-04, -1.8312e-03],\n",
       "          [ 5.7230e-03, -2.7743e-02,  9.3942e-04,  ..., -1.6268e-03,\n",
       "            2.0067e-04, -2.5628e-03],\n",
       "          ...,\n",
       "          [-3.2228e-03,  5.4898e-03,  7.4173e-03,  ...,  5.0559e-03,\n",
       "           -2.2053e-03, -1.2808e-03],\n",
       "          [-5.7643e-04,  3.0170e-03,  6.7040e-05,  ...,  1.1989e-03,\n",
       "            1.2030e-03,  4.4135e-03],\n",
       "          [-2.1686e-03,  5.7898e-03, -1.6913e-03,  ...,  3.7366e-03,\n",
       "            2.6377e-03, -7.4924e-04]],\n",
       "\n",
       "         [[-2.5999e-03,  2.2403e-03, -3.5068e-04,  ..., -3.3393e-03,\n",
       "           -5.3478e-03,  1.6316e-03],\n",
       "          [-1.6861e-02,  8.8377e-03,  6.1746e-03,  ..., -5.8805e-03,\n",
       "            5.0853e-04, -3.1026e-03],\n",
       "          [ 1.7967e-04, -5.1077e-03, -6.1697e-03,  ...,  2.3568e-03,\n",
       "           -2.2931e-03, -2.6975e-03],\n",
       "          ...,\n",
       "          [-4.3771e-03, -1.3781e-02, -2.4263e-03,  ...,  3.4689e-03,\n",
       "           -7.3516e-04, -1.1091e-03],\n",
       "          [-1.7109e-03, -2.6199e-03,  2.0830e-03,  ..., -7.6358e-04,\n",
       "           -3.1370e-03, -6.6187e-05],\n",
       "          [-1.7717e-03,  3.6952e-03, -4.9081e-03,  ...,  3.3826e-03,\n",
       "            1.6585e-03, -8.7176e-04]],\n",
       "\n",
       "         [[ 2.0338e-03,  6.8374e-03,  2.2245e-03,  ..., -1.5477e-03,\n",
       "            3.1048e-03,  1.4997e-03],\n",
       "          [ 1.1397e-03, -7.8773e-03,  7.9788e-04,  ...,  2.2551e-03,\n",
       "            8.1750e-04, -1.3272e-03],\n",
       "          [-1.5382e-02,  6.0750e-03, -1.2570e-02,  ..., -7.3464e-03,\n",
       "           -2.3531e-03,  1.3115e-03],\n",
       "          ...,\n",
       "          [ 4.0944e-03, -3.5695e-03,  4.0459e-03,  ..., -2.0605e-04,\n",
       "            3.4341e-03, -5.6102e-03],\n",
       "          [-1.4600e-03, -4.1698e-03,  5.9322e-03,  ...,  1.8722e-03,\n",
       "           -1.7532e-04, -3.4247e-04],\n",
       "          [ 1.8342e-03,  9.7453e-03,  1.5448e-03,  ...,  3.0362e-03,\n",
       "            4.8719e-03,  2.3192e-04]]],\n",
       "\n",
       "\n",
       "        [[[-6.0544e-03,  4.2011e-03, -1.2202e-02,  ..., -4.1812e-03,\n",
       "           -3.9607e-03,  4.2270e-03],\n",
       "          [-5.3317e-03, -1.7165e-02, -4.0045e-03,  ..., -5.0507e-04,\n",
       "           -4.4626e-03,  4.8668e-03],\n",
       "          [-1.2461e-02,  9.2985e-03,  6.8595e-03,  ..., -1.4782e-02,\n",
       "            6.6110e-03, -1.8462e-03],\n",
       "          ...,\n",
       "          [-9.0138e-03, -1.4873e-03, -2.6000e-04,  ...,  2.1589e-03,\n",
       "            1.8738e-03,  2.1303e-04],\n",
       "          [ 4.4659e-03,  1.2417e-03,  1.1216e-02,  ...,  1.0633e-03,\n",
       "            1.0229e-03, -3.1236e-04],\n",
       "          [ 7.6980e-04,  7.3216e-03,  7.0389e-03,  ..., -2.9225e-04,\n",
       "           -5.1333e-04, -1.8143e-03]],\n",
       "\n",
       "         [[ 3.8886e-03,  4.6307e-03,  1.1665e-02,  ...,  1.3095e-02,\n",
       "            4.8082e-03, -4.1421e-03],\n",
       "          [ 3.7717e-03,  2.0770e-02,  3.0078e-03,  ...,  1.0253e-02,\n",
       "           -4.0097e-03, -6.1030e-03],\n",
       "          [ 2.2752e-03,  1.0428e-05,  6.1406e-03,  ...,  1.2357e-02,\n",
       "            6.0199e-03, -1.8573e-03],\n",
       "          ...,\n",
       "          [ 1.5273e-04,  6.7621e-03,  4.4014e-03,  ..., -5.7488e-04,\n",
       "           -3.1968e-04, -6.0037e-04],\n",
       "          [ 4.3165e-03, -5.1259e-03, -4.8836e-04,  ..., -4.1808e-03,\n",
       "           -7.2681e-04,  3.2804e-04],\n",
       "          [ 5.1414e-03,  2.0239e-03,  7.0404e-03,  ...,  7.4287e-04,\n",
       "            1.8191e-03,  4.2199e-04]],\n",
       "\n",
       "         [[-4.0276e-03, -9.4347e-03,  6.2225e-03,  ...,  5.4127e-03,\n",
       "           -5.9976e-04,  7.8721e-03],\n",
       "          [ 2.5221e-03,  9.7318e-03,  1.1640e-02,  ..., -8.5911e-03,\n",
       "           -7.5057e-03, -3.3781e-03],\n",
       "          [ 9.3301e-03,  1.6336e-03, -5.5493e-03,  ..., -1.2441e-02,\n",
       "            3.0478e-03, -2.5993e-03],\n",
       "          ...,\n",
       "          [-5.5800e-03,  1.4415e-03, -1.0308e-02,  ..., -7.0664e-04,\n",
       "            7.3842e-04, -3.1531e-04],\n",
       "          [ 7.9256e-04, -2.1473e-03, -8.7829e-03,  ..., -1.3601e-03,\n",
       "           -1.4081e-03, -3.6874e-04],\n",
       "          [-7.9651e-03,  1.1132e-02,  8.4843e-04,  ...,  1.1984e-03,\n",
       "            1.9357e-03, -1.1500e-03]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=torch.sum(torch.from_numpy(k).float()*out2)\n",
    "l.backward()\n",
    "in_t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1314,  0.3261, -0.0415])\n",
      "[ 2.219511   -0.45285076 -0.7259828 ]\n"
     ]
    }
   ],
   "source": [
    "print(bn2.left[4].weight.grad)\n",
    "print(record[3]-bn1.path1[4].gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu1 = relu()\n",
    "relu2 = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.8657,  0.7099,  1.3849, -2.6403],\n",
       "          [ 0.7308,  0.8718,  0.2971,  1.6389],\n",
       "          [-0.7588,  0.1648, -0.9381,  0.2895],\n",
       "          [ 2.1914,  1.7482,  0.7808, -2.2425]],\n",
       "\n",
       "         [[-0.2162,  0.9270,  1.2617, -0.3084],\n",
       "          [-1.1653, -0.4890,  1.0565, -1.0408],\n",
       "          [ 0.9162, -0.4509, -0.5819,  0.4063],\n",
       "          [-3.6674, -0.2084, -0.1758,  0.1968]],\n",
       "\n",
       "         [[ 0.5012,  0.0386,  1.1023,  1.4681],\n",
       "          [ 2.4505,  1.7032, -0.0731, -0.8587],\n",
       "          [ 0.0627,  1.3656, -1.3541, -1.7107],\n",
       "          [ 1.4836, -2.0714, -2.0978,  0.8290]]]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_n = np.random.randn(1,3,4,4)\n",
    "in_t = torch.from_numpy(in_n).float()\n",
    "in_t.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.         0.70992776 1.38488918 0.        ]\n",
      "   [0.7308161  0.87184872 0.29706023 1.6389423 ]\n",
      "   [0.         0.16479439 0.         0.28948098]\n",
      "   [2.1914497  1.74815641 0.7808052  0.        ]]\n",
      "\n",
      "  [[0.         0.92701882 1.26173421 0.        ]\n",
      "   [0.         0.         1.05650258 0.        ]\n",
      "   [0.91617213 0.         0.         0.40627932]\n",
      "   [0.         0.         0.         0.19680612]]\n",
      "\n",
      "  [[0.50123578 0.03857217 1.1023445  1.46813431]\n",
      "   [2.45050671 1.70323962 0.         0.        ]\n",
      "   [0.0626919  1.36563698 0.         0.        ]\n",
      "   [1.48359461 0.         0.         0.82895613]]]]\n",
      "tensor([[[[0.0000, 0.7099, 1.3849, 0.0000],\n",
      "          [0.7308, 0.8718, 0.2971, 1.6389],\n",
      "          [0.0000, 0.1648, 0.0000, 0.2895],\n",
      "          [2.1914, 1.7482, 0.7808, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.9270, 1.2617, 0.0000],\n",
      "          [0.0000, 0.0000, 1.0565, 0.0000],\n",
      "          [0.9162, 0.0000, 0.0000, 0.4063],\n",
      "          [0.0000, 0.0000, 0.0000, 0.1968]],\n",
      "\n",
      "         [[0.5012, 0.0386, 1.1023, 1.4681],\n",
      "          [2.4505, 1.7032, 0.0000, 0.0000],\n",
      "          [0.0627, 1.3656, 0.0000, 0.0000],\n",
      "          [1.4836, 0.0000, 0.0000, 0.8290]]]], grad_fn=<ThresholdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out1 = relu1.forward(in_n)\n",
    "print(out1)\n",
    "out2 = relu2(in_t)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0. 1. 1. 0.]\n",
      "   [1. 1. 1. 1.]\n",
      "   [0. 1. 0. 1.]\n",
      "   [1. 1. 1. 0.]]\n",
      "\n",
      "  [[0. 1. 1. 0.]\n",
      "   [0. 0. 1. 0.]\n",
      "   [1. 0. 0. 1.]\n",
      "   [0. 0. 0. 1.]]\n",
      "\n",
      "  [[1. 1. 1. 1.]\n",
      "   [1. 1. 0. 0.]\n",
      "   [1. 1. 0. 0.]\n",
      "   [1. 0. 0. 1.]]]]\n"
     ]
    }
   ],
   "source": [
    "relu1.backward(np.ones(list(out1.shape)))\n",
    "print(relu1.in_diff_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 1., 1., 0.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [0., 1., 0., 1.],\n",
      "          [1., 1., 1., 0.]],\n",
      "\n",
      "         [[0., 1., 1., 0.],\n",
      "          [0., 0., 1., 0.],\n",
      "          [1., 0., 0., 1.],\n",
      "          [0., 0., 0., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 0., 0.],\n",
      "          [1., 1., 0., 0.],\n",
      "          [1., 0., 0., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "l = torch.sum(out2)\n",
    "l.backward()\n",
    "print(in_t.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch4]",
   "language": "python",
   "name": "conda-env-pytorch4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
