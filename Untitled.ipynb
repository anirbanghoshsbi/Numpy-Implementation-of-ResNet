{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import  functional as F\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    '''\n",
    "    实现子module: Residual Block\n",
    "    '''\n",
    "    def __init__(self, inchannel, outchannel, stride=1, shortcut=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "                nn.Conv2d(inchannel,outchannel,3,stride, 1,bias=False),\n",
    "                nn.BatchNorm2d(outchannel),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(outchannel,outchannel,3,1,1,bias=False),\n",
    "                nn.BatchNorm2d(outchannel) )\n",
    "        self.right = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        residual = x if self.right is None else self.right(x)\n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "\n",
    "    def save(self, dir, conv_num, bn_num):\n",
    "        conv1 = self.left[0].weight.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"conv{}_weight.npy\".format(str(conv_num))), conv1)\n",
    "        conv_num += 1\n",
    "\n",
    "        bn1w = self.left[1].weight.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_weight.npy\".format(str(bn_num))), bn1w)\n",
    "        bn1b = self.left[1].bias.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_bias.npy\".format(str(bn_num))), bn1b)\n",
    "        bn1m = self.left[1].running_mean.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_mean.npy\".format(str(bn_num))), bn1m)\n",
    "        bn1v = self.left[1].running_var.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_var.npy\".format(str(bn_num))), bn1v)\n",
    "        bn_num += 1\n",
    "\n",
    "        conv2 = self.left[3].weight.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"conv{}_weight.npy\".format(str(conv_num))), conv2)\n",
    "        conv_num += 1\n",
    "\n",
    "        bn2w = self.left[4].weight.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_weight.npy\".format(str(bn_num))), bn2w)\n",
    "        bn2b = self.left[4].bias.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_bias.npy\".format(str(bn_num))), bn2b)\n",
    "        bn2m = self.left[4].running_mean.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_mean.npy\".format(str(bn_num))), bn2m)\n",
    "        bn2v = self.left[4].running_var.data.detach().cpu().numpy()\n",
    "        np.save(os.path.join(dir, \"bn{}_var.npy\".format(str(bn_num))), bn2v)\n",
    "        bn_num += 1\n",
    "\n",
    "        if self.right is not None:\n",
    "            conv3 = self.right[0].weight.data.detach().cpu().numpy()\n",
    "            np.save(os.path.join(dir, \"conv{}_weight.npy\".format(str(conv_num))), conv3)\n",
    "            conv_num += 1\n",
    "\n",
    "            bn3w = self.right[1].weight.data.detach().cpu().numpy()\n",
    "            np.save(os.path.join(dir, \"bn{}_weight.npy\".format(str(bn_num))), bn3w)\n",
    "            bn3b = self.right[1].bias.data.detach().cpu().numpy()\n",
    "            np.save(os.path.join(dir, \"bn{}_bias.npy\".format(str(bn_num))), bn3b)\n",
    "            bn3m = self.right[1].running_mean.data.detach().cpu().numpy()\n",
    "            np.save(os.path.join(dir, \"bn{}_mean.npy\".format(str(bn_num))), bn3m)\n",
    "            bn3v = self.right[1].running_var.data.detach().cpu().numpy()\n",
    "            np.save(os.path.join(dir, \"bn{}_var.npy\".format(str(bn_num))), bn3v)\n",
    "            bn_num += 1\n",
    "\n",
    "        return conv_num, bn_num\n",
    "    \n",
    "    def load(self, dir, conv_num, bn_num):\n",
    "        conv1 = np.load(os.path.join(dir, \"conv{}_weight.npy\".format(conv_num)))\n",
    "        self.left[0].weight.data = torch.from_numpy(conv1)\n",
    "        conv_num += 1\n",
    "\n",
    "        bn1w = np.load(os.path.join(dir, \"bn{}_weight.npy\".format(bn_num)))\n",
    "        self.left[1].weight.data = torch.from_numpy(bn1w)\n",
    "        bn1b = np.load(os.path.join(dir, \"bn{}_bias.npy\".format(bn_num)))\n",
    "        self.left[1].bias.data = torch.from_numpy(bn1b)\n",
    "        bn1m = np.load(os.path.join(dir, \"bn{}_mean.npy\".format(bn_num)))\n",
    "        self.left[1].running_mean.data = torch.from_numpy(bn1m)\n",
    "        bn1v = np.load(os.path.join(dir, \"bn{}_var.npy\".format(bn_num)))\n",
    "        self.left[1].running_var.data = torch.from_numpy(bn1v)\n",
    "        bn_num += 1\n",
    "\n",
    "        conv2 = np.load(os.path.join(dir, \"conv{}_weight.npy\".format(conv_num)))\n",
    "        self.left[3].weight.data = torch.from_numpy(conv2)\n",
    "        conv_num += 1\n",
    "\n",
    "        bn2w = np.load(os.path.join(dir, \"bn{}_weight.npy\".format(bn_num)))\n",
    "        self.left[4].weight.data = torch.from_numpy(bn2w)\n",
    "        bn2b = np.load(os.path.join(dir, \"bn{}_bias.npy\".format(bn_num)))\n",
    "        self.left[4].bias.data = torch.from_numpy(bn2b)\n",
    "        bn2m = np.load(os.path.join(dir, \"bn{}_mean.npy\".format(bn_num)))\n",
    "        self.left[4].running_mean.data = torch.from_numpy(bn2m)\n",
    "        bn2v = np.load(os.path.join(dir, \"bn{}_var.npy\".format(bn_num)))\n",
    "        self.left[4].running_var.data = torch.from_numpy(bn2v)\n",
    "        bn_num += 1\n",
    "\n",
    "        if self.right is not None:\n",
    "            conv3 = np.load(os.path.join(dir, \"conv{}_weight.npy\".format(conv_num)))\n",
    "            self.right[0].weight.data = torch.from_numpy(conv3)\n",
    "            conv_num += 1\n",
    "\n",
    "            bn3w = np.load(os.path.join(dir, \"bn{}_weight.npy\".format(bn_num)))\n",
    "            self.right[1].weight.data = torch.from_numpy(bn3w)\n",
    "            bn3b = np.load(os.path.join(dir, \"bn{}_bias.npy\".format(bn_num)))\n",
    "            self.right[1].bias.data = torch.from_numpy(bn3b)\n",
    "            bn3m = np.load(os.path.join(dir, \"bn{}_mean.npy\".format(bn_num)))\n",
    "            self.right[1].running_mean.data = torch.from_numpy(bn3m)\n",
    "            bn3v = np.load(os.path.join(dir, \"bn{}_var.npy\".format(bn_num)))\n",
    "            self.right[1].running_var.data = torch.from_numpy(bn3v)\n",
    "            bn_num += 1\n",
    "\n",
    "        return conv_num, bn_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_layer:\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_h, kernel_w, same = True, stride = 1, shift = True):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_h = kernel_h\n",
    "        self.kernel_w = kernel_w\n",
    "        self.same = same\n",
    "        self.stride = stride\n",
    "        self.shift = shift\n",
    "\n",
    "        self.init_param()\n",
    "\n",
    "    def init_param(self):\n",
    "        self.kernel = np.random.uniform(\n",
    "            low = -np.sqrt(6.0/(self.out_channels + self.in_channels * self.kernel_h * self.kernel_w)),\n",
    "            high = np.sqrt(6.0/(self.in_channels + self.out_channels * self.kernel_h * self.kernel_w)),\n",
    "            size = (self.out_channels, self.in_channels, self.kernel_h, self.kernel_w)\n",
    "        )\n",
    "        self.bias = np.zeros([self.out_channels]) if self.shift else None\n",
    "\n",
    "    @staticmethod\n",
    "    def pad(in_tensor, pad_h, pad_w):\n",
    "        batch_num = in_tensor.shape[0]\n",
    "        in_channels = in_tensor.shape[1]\n",
    "        in_h = in_tensor.shape[2]\n",
    "        in_w = in_tensor.shape[3]\n",
    "        padded = np.zeros([batch_num, in_channels, in_h + 2*pad_h, in_w + 2*pad_w])\n",
    "        padded[:, :, pad_h:pad_h+in_h, pad_w:pad_w+in_w] = in_tensor\n",
    "        return padded\n",
    "    \n",
    "    @staticmethod\n",
    "    def convolution(in_tensor, kernel, stride = 1, dilate = 1):\n",
    "        batch_num = in_tensor.shape[0]\n",
    "        in_channels = in_tensor.shape[1]\n",
    "        in_h = in_tensor.shape[2]\n",
    "        in_w = in_tensor.shape[3]\n",
    "        out_channels = kernel.shape[0]\n",
    "        assert kernel.shape[1] == in_channels\n",
    "        kernel_h = kernel.shape[2]\n",
    "        kernel_w = kernel.shape[3]\n",
    "        \n",
    "        out_h = int((in_h - kernel_h + 1)/stride)\n",
    "        out_w = int((in_w - kernel_w + 1)/stride)\n",
    "        \n",
    "        kernel = kernel.reshape(out_channels, -1)\n",
    "        \n",
    "        extend_in = np.zeros([in_channels*kernel_h*kernel_w, batch_num*out_h*out_w])\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                part_in = in_tensor[:, :, i*stride:i*stride+kernel_h, j*stride:j*stride+kernel_w].reshape(batch_num, -1)\n",
    "                extend_in[:, (i*out_w+j)*batch_num:(i*out_w+j+1)*batch_num] = part_in.T\n",
    "        \n",
    "        out_tensor = np.dot(kernel, extend_in)\n",
    "        out_tensor = out_tensor.reshape(out_channels, out_h*out_w, batch_num)\n",
    "        out_tensor = out_tensor.transpose(2,0,1).reshape(batch_num, out_channels, out_h, out_w) \n",
    "        \n",
    "        return out_tensor\n",
    "    \n",
    "    def forward(self, in_tensor):\n",
    "        if self.same:\n",
    "            in_tensor = conv_layer.pad(in_tensor, int((self.kernel_h-1)/2), int((self.kernel_w-1)/2))\n",
    "        \n",
    "        self.in_tensor = in_tensor.copy()\n",
    "        \n",
    "        self.out_tensor = conv_layer.convolution(in_tensor, self.kernel, self.stride)\n",
    "\n",
    "        if self.shift:\n",
    "            self.out_tensor += self.bias.reshape(1,self.out_channels,1,1)\n",
    "\n",
    "        return self.out_tensor\n",
    "    \n",
    "    def backward(self, out_diff_tensor, lr):\n",
    "        assert out_diff_tensor.shape == self.out_tensor.shape\n",
    "\n",
    "        if self.shift:\n",
    "            bias_diff = np.sum(out_diff_tensor, axis = (0,2,3)).reshape(self.bias.shape)\n",
    "            self.bias -= lr * bias_diff\n",
    "        \n",
    "        batch_num = out_diff_tensor.shape[0]\n",
    "        out_channels = out_diff_tensor.shape[1]\n",
    "        out_h = out_diff_tensor.shape[2]\n",
    "        out_w = out_diff_tensor.shape[3]\n",
    "        extend_out = np.zeros([batch_num, out_channels, out_h, out_w, self.stride * self.stride])\n",
    "        extend_out[:, :, :, :, 0] = out_diff_tensor\n",
    "        extend_out = extend_out.reshape(batch_num, out_channels, out_h, out_w, self.stride, self.stride)\n",
    "        extend_out = extend_out.transpose(0,1,2,4,3,5).reshape(batch_num, out_channels, out_h*self.stride, out_w*self.stride)\n",
    "\n",
    "        kernel_diff = conv_layer.convolution(self.in_tensor.transpose(1,0,2,3), extend_out.transpose(1,0,2,3))\n",
    "        kernel_diff = kernel_diff.transpose(1,0,2,3)\n",
    "        \n",
    "        padded = conv_layer.pad(extend_out, self.kernel_h-1, self.kernel_w-1)\n",
    "        kernel_trans = self.kernel.reshape(self.out_channels, self.in_channels, self.kernel_h*self.kernel_w)\n",
    "        kernel_trans = kernel_trans[:,:,::-1].reshape(self.kernel.shape)\n",
    "        self.in_diff_tensor = conv_layer.convolution(padded, kernel_trans.transpose(1,0,2,3))\n",
    "        if self.same:\n",
    "            pad_h = int((self.kernel_h-1)/2)\n",
    "            pad_w = int((self.kernel_w-1)/2)\n",
    "            self.in_diff_tensor = self.in_diff_tensor[:, :, pad_h:-pad_h, pad_w:-pad_w]\n",
    "            \n",
    "        self.kernel -= lr * kernel_diff\n",
    "\n",
    "    def save(self, path, conv_num):\n",
    "        if os.path.exists(path) == False:\n",
    "            os.mkdir(path)\n",
    "\n",
    "        np.save(os.path.join(path, \"conv{}_weight.npy\".format(conv_num)), self.kernel)\n",
    "        if self.shift:\n",
    "            np.save(os.path.join(path, \"conv{}_bias.npy\".format(conv_num)), self.bias)\n",
    "        \n",
    "        return conv_num + 1\n",
    "\n",
    "    def load(self, path, conv_num):\n",
    "        assert os.path.exists(path)\n",
    "\n",
    "        self.kernel = np.load(os.path.join(path, \"conv{}_weight.npy\".format(conv_num)))\n",
    "        if self.shift:\n",
    "            self.bias = np.load(os.path.join(path, \"conv{}_bias.npy\").format(conv_num))\n",
    "        \n",
    "        return conv_num + 1\n",
    "    \n",
    "class relu:\n",
    "\n",
    "    def forward(self, in_tensor):\n",
    "        self.in_tensor = in_tensor.copy()\n",
    "        self.out_tensor = in_tensor.copy()\n",
    "        self.out_tensor[self.in_tensor < 0.0] = 0.0\n",
    "        return self.out_tensor\n",
    "\n",
    "    def backward(self, out_diff_tensor, lr = 0):\n",
    "        assert self.out_tensor.shape == out_diff_tensor.shape\n",
    "        self.in_diff_tensor = out_diff_tensor.copy()\n",
    "        self.in_diff_tensor[self.in_tensor < 0.0] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "class bn_layer:\n",
    "\n",
    "    def __init__(self, neural_num, moving_rate = 0.1):\n",
    "        self.gamma = np.random.uniform(low=0, high=1, size=neural_num)\n",
    "        self.bias = np.zeros([neural_num])\n",
    "        self.moving_avg = np.zeros([neural_num])\n",
    "        self.moving_var = np.ones([neural_num])\n",
    "        self.neural_num = neural_num\n",
    "        self.moving_rate = moving_rate\n",
    "        self.is_train = True\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def train(self):\n",
    "        self.is_train = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.is_train = False\n",
    "\n",
    "    def forward(self, in_tensor):\n",
    "        assert in_tensor.shape[1] == self.neural_num\n",
    "\n",
    "        self.in_tensor = in_tensor.copy()\n",
    "\n",
    "        if self.is_train:\n",
    "            mean = in_tensor.mean(axis=(0,2,3))\n",
    "            var = in_tensor.var(axis=(0,2,3))\n",
    "            self.moving_avg = mean * self.moving_rate + (1 - self.moving_rate) * self.moving_avg\n",
    "            self.moving_var = var * self.moving_rate + (1 - self.moving_rate) * self.moving_var\n",
    "            self.var = var\n",
    "            self.mean = mean\n",
    "        else:\n",
    "            mean = self.moving_avg\n",
    "            var = self.moving_var\n",
    "\n",
    "        self.normalized = (in_tensor - mean.reshape(1,-1,1,1)) / np.sqrt(var.reshape(1,-1,1,1)+self.epsilon)\n",
    "        out_tensor = self.gamma.reshape(1,-1,1,1) * self.normalized + self.bias.reshape(1,-1,1,1)\n",
    "\n",
    "        return out_tensor\n",
    "\n",
    "    def backward(self, out_diff_tensor, lr):\n",
    "        assert out_diff_tensor.shape == self.in_tensor.shape\n",
    "        assert self.is_train\n",
    "\n",
    "        m = self.in_tensor.shape[0] * self.in_tensor.shape[2] * self.in_tensor.shape[3]\n",
    "\n",
    "        normalized_diff = self.gamma.reshape(1,-1,1,1) * out_diff_tensor\n",
    "        var_diff = -0.5 * np.sum(normalized_diff*self.normalized, axis=(0,2,3)) / (self.var + self.epsilon)\n",
    "        mean_diff = -1.0 * np.sum(normalized_diff, axis=(0,2,3)) / np.sqrt(self.var + self.epsilon)\n",
    "        in_diff_tensor1 = normalized_diff / np.sqrt(self.var.reshape(1,-1,1,1)+self.epsilon)\n",
    "        in_diff_tensor2 = var_diff.reshape(1,-1,1,1) * (self.in_tensor - self.mean.reshape(1,-1,1,1)) * 2 / m\n",
    "        in_diff_tensor3 = mean_diff.reshape(1,-1,1,1) / m\n",
    "        self.in_diff_tensor = in_diff_tensor1 + in_diff_tensor2 + in_diff_tensor3\n",
    "\n",
    "        gamma_diff = np.sum(self.normalized * out_diff_tensor, axis=(0,2,3))\n",
    "        self.gamma -= lr * gamma_diff\n",
    "\n",
    "        bias_diff = np.sum(out_diff_tensor, axis=(0,2,3))\n",
    "        self.bias -= lr * bias_diff \n",
    "\n",
    "    def save(self, path, bn_num):\n",
    "        if os.path.exists(path) == False:\n",
    "            os.mkdir(path)\n",
    "\n",
    "        np.save(os.path.join(path, \"bn{}_weight.npy\".format(bn_num)), self.gamma)\n",
    "        np.save(os.path.join(path, \"bn{}_bias.npy\".format(bn_num)), self.bias)\n",
    "        np.save(os.path.join(path, \"bn{}_mean.npy\".format(bn_num)), self.moving_avg)\n",
    "        np.save(os.path.join(path, \"bn{}_var.npy\".format(bn_num)), self.moving_var)\n",
    "\n",
    "        return bn_num + 1\n",
    "\n",
    "    def load(self, path, bn_num):\n",
    "        assert os.path.exists(path)\n",
    "\n",
    "        self.gamma = np.load(os.path.join(path, \"bn{}_weight.npy\".format(bn_num)))\n",
    "        self.bias = np.load(os.path.join(path, \"bn{}_bias.npy\".format(bn_num)))\n",
    "        self.moving_avg = np.load(os.path.join(path, \"bn{}_mean.npy\".format(bn_num)))\n",
    "        self.moving_var = np.load(os.path.join(path, \"bn{}_var.npy\".format(bn_num)))\n",
    "\n",
    "        return bn_num + 1\n",
    "\n",
    "class ResBlock:\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, shortcut=None):\n",
    "        self.path1 = [\n",
    "            conv_layer(in_channels, out_channels, 3, 3, stride = stride, shift=False),\n",
    "            bn_layer(out_channels),\n",
    "            relu(),\n",
    "            conv_layer(out_channels, out_channels, 3, 3, shift=False),\n",
    "            bn_layer(out_channels)\n",
    "        ]\n",
    "        self.path2 = shortcut\n",
    "        self.relu = relu()\n",
    "    \n",
    "    def train(self):\n",
    "        self.path1[1].train()\n",
    "        self.path1[4].train()\n",
    "        if self.path2 is not None:\n",
    "            self.path2[1].train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.path1[1].eval()\n",
    "        self.path1[4].eval()\n",
    "        if self.path2 is not None:\n",
    "            self.path2[1].eval()\n",
    "\n",
    "    def forward(self, in_tensor):\n",
    "        x1 = in_tensor.copy()\n",
    "        x2 = in_tensor.copy()\n",
    "\n",
    "        for l in self.path1:\n",
    "            x1 = l.forward(x1)\n",
    "        if self.path2 is not None:\n",
    "            for l in self.path2:\n",
    "                x2 = l.forward(x2)\n",
    "        self.out_tensor = self.relu.forward(x1+x2)\n",
    "\n",
    "        return self.out_tensor\n",
    "\n",
    "\n",
    "    def save(self, path, conv_num, bn_num):\n",
    "        conv_num = self.path1[0].save(path, conv_num)\n",
    "        bn_num = self.path1[1].save(path, bn_num)\n",
    "        conv_num = self.path1[3].save(path, conv_num)\n",
    "        bn_num = self.path1[4].save(path, bn_num)\n",
    "\n",
    "        if self.path2 is not None:\n",
    "            conv_num = self.path2[0].save(path, conv_num)\n",
    "            bn_num = self.path2[1].save(path, bn_num)\n",
    "\n",
    "        return conv_num, bn_num\n",
    "\n",
    "    def load(self, path, conv_num, bn_num):\n",
    "        conv_num = self.path1[0].load(path, conv_num)\n",
    "        bn_num = self.path1[1].load(path, bn_num)\n",
    "        conv_num = self.path1[3].load(path, conv_num)\n",
    "        bn_num = self.path1[4].load(path, bn_num)\n",
    "\n",
    "        if self.path2 is not None:\n",
    "            conv_num = self.path2[0].load(path, conv_num)\n",
    "            bn_num = self.path2[1].load(path, bn_num)\n",
    "\n",
    "        return conv_num, bn_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from components import *\n",
    "from network import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_n = np.random.randn(1,3,64,64)\n",
    "in_t = torch.from_numpy(in_n).float()\n",
    "in_t.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1 = resnet34(20)\n",
    "bn2 = ResNet(20)\n",
    "bn2.save(\"model7\")\n",
    "bn1.load(\"model7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.55179711 0.62850806 0.46351128 0.53471551 0.5996611  0.69506106\n",
      "  0.55390215 0.47865034 0.53811183 0.52384631 0.43286357 0.45305416\n",
      "  0.59751233 0.39252624 0.5086335  0.41345826 0.46784149 0.40557694\n",
      "  0.47077887 0.49676474]]\n",
      "tensor([[0.5518, 0.6285, 0.4635, 0.5347, 0.5997, 0.6951, 0.5539, 0.4787, 0.5381,\n",
      "         0.5238, 0.4329, 0.4531, 0.5975, 0.3925, 0.5086, 0.4135, 0.4678, 0.4056,\n",
      "         0.4708, 0.4968]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyzustc/applications/anaconda3/envs/pytorch4/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "out1 = bn1.forward(in_n)\n",
    "out2 = bn2(in_t)\n",
    "print(out1)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512, 2, 2)\n",
      "(1, 256, 4, 4)\n",
      "(1, 256, 4, 4)\n",
      "(1, 128, 8, 8)\n",
      "(1, 128, 8, 8)\n",
      "(1, 64, 16, 16)\n",
      "(1, 64, 16, 16)\n",
      "(1, 64, 16, 16)\n",
      "[[[[ 0.00577287  0.02294563 -0.006909   ...  0.01322922  0.00705284\n",
      "    -0.00463878]\n",
      "   [-0.02594649 -0.01872343 -0.02370644 ...  0.01064385 -0.00896892\n",
      "     0.00728494]\n",
      "   [ 0.04131772  0.00571123 -0.00521393 ... -0.00249198 -0.02890325\n",
      "    -0.01334761]\n",
      "   ...\n",
      "   [ 0.01894822 -0.00025722  0.00390623 ...  0.0008131   0.00965644\n",
      "     0.0012274 ]\n",
      "   [ 0.00444836  0.00805811 -0.00940205 ...  0.0004689   0.00313075\n",
      "    -0.00323602]\n",
      "   [-0.01153335  0.00573831 -0.00508022 ...  0.00617271  0.00439609\n",
      "    -0.00209484]]\n",
      "\n",
      "  [[-0.0160147  -0.01901795  0.00059748 ...  0.01078757 -0.00144316\n",
      "    -0.01155735]\n",
      "   [ 0.03009323  0.00403906  0.02832374 ...  0.01577126  0.00292128\n",
      "     0.00123267]\n",
      "   [-0.01160758  0.01915891 -0.01160553 ...  0.01062682 -0.01701021\n",
      "    -0.00564884]\n",
      "   ...\n",
      "   [ 0.00717421 -0.00848198 -0.02485541 ...  0.0073335  -0.00055931\n",
      "     0.00092504]\n",
      "   [ 0.00014297  0.01340997 -0.00650484 ... -0.00880217 -0.00728646\n",
      "    -0.00213586]\n",
      "   [ 0.00083425  0.00944014 -0.0080715  ...  0.0114918  -0.0037329\n",
      "     0.0005663 ]]\n",
      "\n",
      "  [[ 0.01581166  0.03030242  0.02665257 ...  0.00282011  0.01310452\n",
      "    -0.00130128]\n",
      "   [-0.02392178 -0.01094871  0.01046309 ...  0.00728876 -0.00996982\n",
      "     0.00055919]\n",
      "   [ 0.01074596 -0.03110341  0.03831685 ...  0.00378223  0.01641307\n",
      "    -0.0028845 ]\n",
      "   ...\n",
      "   [ 0.00269112  0.0127608   0.00685964 ...  0.01566984  0.00927813\n",
      "    -0.00432819]\n",
      "   [-0.000805    0.00871706 -0.0020903  ...  0.00120029  0.01149656\n",
      "    -0.00446886]\n",
      "   [-0.00389001  0.00534423 -0.00160748 ... -0.00403363  0.00168768\n",
      "    -0.00175157]]]]\n"
     ]
    }
   ],
   "source": [
    "k = np.random.uniform(0,1,out1.shape)\n",
    "bn1.backward(k,1)\n",
    "print(bn1.in_diff_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0058,  0.0230, -0.0068,  ...,  0.0132,  0.0070, -0.0046],\n",
       "          [-0.0259, -0.0187, -0.0237,  ...,  0.0106, -0.0089,  0.0073],\n",
       "          [ 0.0413,  0.0056, -0.0053,  ..., -0.0026, -0.0288, -0.0134],\n",
       "          ...,\n",
       "          [ 0.0190, -0.0002,  0.0039,  ...,  0.0008,  0.0097,  0.0012],\n",
       "          [ 0.0045,  0.0080, -0.0094,  ...,  0.0005,  0.0031, -0.0032],\n",
       "          [-0.0115,  0.0057, -0.0051,  ...,  0.0062,  0.0044, -0.0021]],\n",
       "\n",
       "         [[-0.0160, -0.0190,  0.0006,  ...,  0.0107, -0.0015, -0.0116],\n",
       "          [ 0.0301,  0.0039,  0.0284,  ...,  0.0158,  0.0029,  0.0013],\n",
       "          [-0.0115,  0.0191, -0.0116,  ...,  0.0106, -0.0170, -0.0057],\n",
       "          ...,\n",
       "          [ 0.0072, -0.0085, -0.0249,  ...,  0.0073, -0.0005,  0.0009],\n",
       "          [ 0.0001,  0.0134, -0.0064,  ..., -0.0088, -0.0073, -0.0021],\n",
       "          [ 0.0009,  0.0094, -0.0081,  ...,  0.0115, -0.0037,  0.0005]],\n",
       "\n",
       "         [[ 0.0158,  0.0303,  0.0266,  ...,  0.0028,  0.0131, -0.0013],\n",
       "          [-0.0239, -0.0109,  0.0105,  ...,  0.0073, -0.0099,  0.0005],\n",
       "          [ 0.0108, -0.0310,  0.0382,  ...,  0.0038,  0.0165, -0.0029],\n",
       "          ...,\n",
       "          [ 0.0027,  0.0128,  0.0068,  ...,  0.0156,  0.0093, -0.0043],\n",
       "          [-0.0008,  0.0087, -0.0021,  ...,  0.0012,  0.0115, -0.0045],\n",
       "          [-0.0039,  0.0053, -0.0016,  ..., -0.0041,  0.0017, -0.0018]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=torch.sum(torch.from_numpy(k).float()*out2)\n",
    "l.backward()\n",
    "in_t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1314,  0.3261, -0.0415])\n",
      "[ 2.219511   -0.45285076 -0.7259828 ]\n"
     ]
    }
   ],
   "source": [
    "print(bn2.left[4].weight.grad)\n",
    "print(record[3]-bn1.path1[4].gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu1 = relu()\n",
    "relu2 = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.8657,  0.7099,  1.3849, -2.6403],\n",
       "          [ 0.7308,  0.8718,  0.2971,  1.6389],\n",
       "          [-0.7588,  0.1648, -0.9381,  0.2895],\n",
       "          [ 2.1914,  1.7482,  0.7808, -2.2425]],\n",
       "\n",
       "         [[-0.2162,  0.9270,  1.2617, -0.3084],\n",
       "          [-1.1653, -0.4890,  1.0565, -1.0408],\n",
       "          [ 0.9162, -0.4509, -0.5819,  0.4063],\n",
       "          [-3.6674, -0.2084, -0.1758,  0.1968]],\n",
       "\n",
       "         [[ 0.5012,  0.0386,  1.1023,  1.4681],\n",
       "          [ 2.4505,  1.7032, -0.0731, -0.8587],\n",
       "          [ 0.0627,  1.3656, -1.3541, -1.7107],\n",
       "          [ 1.4836, -2.0714, -2.0978,  0.8290]]]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_n = np.random.randn(1,3,4,4)\n",
    "in_t = torch.from_numpy(in_n).float()\n",
    "in_t.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.         0.70992776 1.38488918 0.        ]\n",
      "   [0.7308161  0.87184872 0.29706023 1.6389423 ]\n",
      "   [0.         0.16479439 0.         0.28948098]\n",
      "   [2.1914497  1.74815641 0.7808052  0.        ]]\n",
      "\n",
      "  [[0.         0.92701882 1.26173421 0.        ]\n",
      "   [0.         0.         1.05650258 0.        ]\n",
      "   [0.91617213 0.         0.         0.40627932]\n",
      "   [0.         0.         0.         0.19680612]]\n",
      "\n",
      "  [[0.50123578 0.03857217 1.1023445  1.46813431]\n",
      "   [2.45050671 1.70323962 0.         0.        ]\n",
      "   [0.0626919  1.36563698 0.         0.        ]\n",
      "   [1.48359461 0.         0.         0.82895613]]]]\n",
      "tensor([[[[0.0000, 0.7099, 1.3849, 0.0000],\n",
      "          [0.7308, 0.8718, 0.2971, 1.6389],\n",
      "          [0.0000, 0.1648, 0.0000, 0.2895],\n",
      "          [2.1914, 1.7482, 0.7808, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.9270, 1.2617, 0.0000],\n",
      "          [0.0000, 0.0000, 1.0565, 0.0000],\n",
      "          [0.9162, 0.0000, 0.0000, 0.4063],\n",
      "          [0.0000, 0.0000, 0.0000, 0.1968]],\n",
      "\n",
      "         [[0.5012, 0.0386, 1.1023, 1.4681],\n",
      "          [2.4505, 1.7032, 0.0000, 0.0000],\n",
      "          [0.0627, 1.3656, 0.0000, 0.0000],\n",
      "          [1.4836, 0.0000, 0.0000, 0.8290]]]], grad_fn=<ThresholdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out1 = relu1.forward(in_n)\n",
    "print(out1)\n",
    "out2 = relu2(in_t)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0. 1. 1. 0.]\n",
      "   [1. 1. 1. 1.]\n",
      "   [0. 1. 0. 1.]\n",
      "   [1. 1. 1. 0.]]\n",
      "\n",
      "  [[0. 1. 1. 0.]\n",
      "   [0. 0. 1. 0.]\n",
      "   [1. 0. 0. 1.]\n",
      "   [0. 0. 0. 1.]]\n",
      "\n",
      "  [[1. 1. 1. 1.]\n",
      "   [1. 1. 0. 0.]\n",
      "   [1. 1. 0. 0.]\n",
      "   [1. 0. 0. 1.]]]]\n"
     ]
    }
   ],
   "source": [
    "relu1.backward(np.ones(list(out1.shape)))\n",
    "print(relu1.in_diff_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 1., 1., 0.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [0., 1., 0., 1.],\n",
      "          [1., 1., 1., 0.]],\n",
      "\n",
      "         [[0., 1., 1., 0.],\n",
      "          [0., 0., 1., 0.],\n",
      "          [1., 0., 0., 1.],\n",
      "          [0., 0., 0., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 0., 0.],\n",
      "          [1., 1., 0., 0.],\n",
      "          [1., 0., 0., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "l = torch.sum(out2)\n",
    "l.backward()\n",
    "print(in_t.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch4]",
   "language": "python",
   "name": "conda-env-pytorch4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
