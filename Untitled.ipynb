{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self, in_channels, out_channels, kernel_h, kernel_w):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_h = kernel_h\n",
    "        self.kernel_w = kernel_w\n",
    "        self.kernel = np.zeros([out_channels, in_channels, kernel_h, kernel_w])\n",
    "        self.bias = np.zeros([out_channels, 1])\n",
    "class conv_layer(layer):    \n",
    "    def __init__(self, in_channels, out_channels, kernel_h, kernel_w, same = True, relu = True, shift = True):\n",
    "        layer.__init__(self, in_channels, out_channels, kernel_h, kernel_w)\n",
    "        self.init_param()\n",
    "        self.same = same\n",
    "        self.relu = relu\n",
    "        self.shift = shift\n",
    "\n",
    "    def init_param(self):\n",
    "        #self.kernel = np.random.randn(self.out_channels, self.in_channels, self.kernel_h, self.kernel_w)*0.01\n",
    "        self.kernel += 1\n",
    "    @staticmethod\n",
    "    def pad(in_tensor, pad_h, pad_w):\n",
    "        batch_num = in_tensor.shape[0]\n",
    "        in_channels = in_tensor.shape[1]\n",
    "        in_h = in_tensor.shape[2]\n",
    "        in_w = in_tensor.shape[3]\n",
    "        padded = np.zeros([batch_num, in_channels, in_h + 2*pad_h, in_w + 2*pad_w])\n",
    "        padded[:, :, pad_h:pad_h+in_h, pad_w:pad_w+in_w] = in_tensor\n",
    "        return padded\n",
    "    \n",
    "    @staticmethod\n",
    "    def convolution(in_tensor, kernel):\n",
    "        batch_num = in_tensor.shape[0]\n",
    "        in_channels = in_tensor.shape[1]\n",
    "        in_h = in_tensor.shape[2]\n",
    "        in_w = in_tensor.shape[3]\n",
    "        out_channels = kernel.shape[0]\n",
    "        assert kernel.shape[1] == in_channels\n",
    "        kernel_h = kernel.shape[2]\n",
    "        kernel_w = kernel.shape[3]\n",
    "        \n",
    "        out_h = in_h - kernel_h + 1\n",
    "        out_w = in_w - kernel_w + 1\n",
    "        \n",
    "        kernel = kernel.reshape(out_channels, -1)\n",
    "        \n",
    "        extend_in = np.zeros([in_channels*kernel_h*kernel_w, batch_num*out_h*out_w])\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                part_in = in_tensor[:, :, i:i+kernel_h, j:j+kernel_w].reshape(batch_num, -1)\n",
    "                extend_in[:, (i*out_w+j)*batch_num:(i*out_w+j+1)*batch_num] = part_in.T\n",
    "        \n",
    "        out_tensor = np.dot(kernel, extend_in)\n",
    "        out_tensor = out_tensor.reshape(out_channels, out_h*out_w, batch_num)\n",
    "        out_tensor = out_tensor.transpose(2,0,1).reshape(batch_num, out_channels, out_h, out_w) \n",
    "        \n",
    "        return out_tensor\n",
    "    \n",
    "    def forward(self, in_tensor):\n",
    "        if self.same:\n",
    "            in_tensor = conv_layer.pad(in_tensor, int((self.kernel_h-1)/2), int((self.kernel_w-1)/2))\n",
    "        \n",
    "        self.in_tensor = in_tensor\n",
    "        \n",
    "        self.out_tensor = conv_layer.convolution(in_tensor, self.kernel)\n",
    "\n",
    "        if self.shift:\n",
    "            self.out_tensor += self.bias.reshape(1,self.out_channels,1,1)\n",
    "\n",
    "        out_tensor = self.out_tensor\n",
    "\n",
    "        if self.relu:\n",
    "            out_tensor[out_tensor < 0] = 0\n",
    "\n",
    "        return out_tensor\n",
    "    \n",
    "    def backward(self, out_diff_tensor, lr):\n",
    "        assert out_diff_tensor.shape == self.out_tensor.shape\n",
    "        \n",
    "        if self.relu:\n",
    "            out_diff_tensor[self.out_tensor == 0] = 0\n",
    "        \n",
    "        if self.shift:\n",
    "            bias_diff = np.sum(out_diff_tensor, axis = (0,2,3)).reshape(self.bias.shape)\n",
    "            self.bias -= lr * bias_diff\n",
    "\n",
    "        kernel_diff = conv_layer.convolution(self.in_tensor.transpose(1,0,2,3), out_diff_tensor.transpose(1,0,2,3))\n",
    "        kernel_diff = kernel_diff.transpose(1,0,2,3)\n",
    "        \n",
    "        padded = conv_layer.pad(out_diff_tensor, self.kernel_h-1, self.kernel_w-1)\n",
    "        kernel_trans = self.kernel.reshape(self.out_channels, self.in_channels, self.kernel_h*self.kernel_w)\n",
    "        kernel_trans = kernel_trans[:,:,::-1].reshape(self.kernel.shape)\n",
    "        self.in_diff_tensor = conv_layer.convolution(padded, kernel_trans.transpose(1,0,2,3))\n",
    "        if self.same:\n",
    "            pad_h = int((self.kernel_h-1)/2)\n",
    "            pad_w = int((self.kernel_w-1)/2)\n",
    "            self.in_diff_tensor = self.in_diff_tensor[:, :, pad_h:-pad_h, pad_w:-pad_w]\n",
    "            \n",
    "        \n",
    "        self.kernel -= lr * kernel_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = conv_layer(3,8,3,3)\n",
    "\n",
    "intensor = np.ones([1,3,64,64])\n",
    "out = conv.forward(intensor)\n",
    "out_diff = np.ones(list(out.shape))\n",
    "conv.backward(out_diff,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.  18.  18.  18.  18.  18.  18.  18.]\n",
      " [ 18.  27.  27.  27.  27.  27.  27.  27.]\n",
      " [ 18.  27.  27.  27.  27.  27.  27.  27.]\n",
      " [ 18.  27.  27.  27.  27.  27.  27.  27.]\n",
      " [ 18.  27.  27.  27.  27.  27.  27.  27.]\n",
      " [ 18.  27.  27.  27.  27.  27.  27.  27.]\n",
      " [ 18.  27.  27.  27.  27.  27.  27.  27.]\n",
      " [ 18.  27.  27.  27.  27.  27.  27.  27.]]\n",
      "[[ 32.  48.  48.  48.  48.  48.  48.  48.]\n",
      " [ 48.  72.  72.  72.  72.  72.  72.  72.]\n",
      " [ 48.  72.  72.  72.  72.  72.  72.  72.]\n",
      " [ 48.  72.  72.  72.  72.  72.  72.  72.]\n",
      " [ 48.  72.  72.  72.  72.  72.  72.  72.]\n",
      " [ 48.  72.  72.  72.  72.  72.  72.  72.]\n",
      " [ 48.  72.  72.  72.  72.  72.  72.  72.]\n",
      " [ 48.  72.  72.  72.  72.  72.  72.  72.]]\n",
      "[[-3968. -4031. -3968.]\n",
      " [-4031. -4095. -4031.]\n",
      " [-3968. -4031. -3968.]]\n",
      "[[-4096.]\n",
      " [-4096.]\n",
      " [-4096.]\n",
      " [-4096.]\n",
      " [-4096.]\n",
      " [-4096.]\n",
      " [-4096.]\n",
      " [-4096.]]\n"
     ]
    }
   ],
   "source": [
    "print(out[0,0,0:8,0:8])\n",
    "print(conv.in_diff_tensor[0,0,0:8,0:8])\n",
    "print(conv.kernel[0,0,:,:])\n",
    "print(conv.bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
